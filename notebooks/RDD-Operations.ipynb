{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for PySpark\n",
    "# The variables should configure Spark home, Jupyter as the driver, and Python as the interpreter.\n",
    "# Additionally, ensure that Jupyter notebook is used as the driver environment.\n",
    "import os\n",
    "\n",
    "os.environ['SPARK_HOME'] = 'C:/Users/saulr/anaconda3/envs/pyspark-env/Lib/site-packages/pyspark'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark Session from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('SparkRDD').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDD from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RDD from a list of numbers From 1 to 10\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Perform a collect action to view the data\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDD from a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of tuples with name and age between 20 and 49\n",
    "data = [('John', 25), ('Lisa', 28), ('Saul', 49), ('Maria', 35), ('Lee', 40)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Perform a collect action to view the data\n",
    "rdd.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDDs Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformation: Convert name to uppercase\n",
    "mappedData = rdd.map(lambda x: (x[0].upper(), x[1]))\n",
    "\n",
    "# Perform a collect action to see the data\n",
    "mappedData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter transformation: Filter records with age greater than 30\n",
    "filteredData = rdd.filter(lambda x: x[1] > 30)\n",
    "\n",
    "# Perform a collect action to see the data\n",
    "filteredData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReduceByKey Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('John', 25), ('Lisa', 28), ('Saul', 49), ('Maria', 35), ('Lee', 40), ('John', 30), ('Lisa', 25)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# ReduceByKey: Calculate the total age for each name\n",
    "reducedData = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Perform a collect action to see the data\n",
    "reducedData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SortBy Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SortyBy Transformation: Sort the data by age in descending order\n",
    "sortedData = rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Perform a collect action to see the data\n",
    "sortedData.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
